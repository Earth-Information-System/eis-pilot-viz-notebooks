{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EIS Fire Visualizations Notebook 1 v1.1.1\n",
    "### Version: 06.23.21\n",
    "EIS â€“ Fire Notebook 1: Visualizations Notebooks aims to create interactive visualizations of IMERG and other raster data. These are interactive visualizations of raster data in conjunction with vector layers with added functionality including (but not limited to) time-series averages, animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import s3fs\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import zipfile\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import colorcet as cc\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "from holoviews import opts, streams\n",
    "from holoviews.plotting.links import DataLink\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, DrawControl\n",
    "import metpy.calc as mpc\n",
    "import metpy\n",
    "import panel as pn\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping, Point\n",
    "\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session, sessionmaker, declarative_base\n",
    "from sqlalchemy import create_engine, MetaData, Table, inspect, and_, between, distinct\n",
    "from sqlalchemy import select, func, distinct, between, and_, or_, not_, Integer\n",
    "\n",
    "xr.set_options(display_style=\"html\")\n",
    "warnings.filterwarnings('ignore')\n",
    "pn.extension()\n",
    "pn.param.ParamMethod.loading_indicator = True\n",
    "pn.extension(sizing_mode=\"stretch_width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Add, modify or remove the desired Zarr dataset file. Following the convention:\n",
    "`\"DATASET\" : \"path/to/zarr/file.zarr\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_filepath = {\n",
    "    'IMERG FWI': \"dh-eis-fire-usw2-shared/imerg-fwi.zarr\",\n",
    "    'GEOS FP CONUS': \"dh-eis-fire-usw2-shared/geos-fp-zarr/conus.zarr\",\n",
    "    'QFED': 'dh-eis-fire-usw2-shared/qfed.zarr/',\n",
    "    'GEOS FWI': \"dh-eis-fire-usw2-shared/geos-fwi/zarr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TROPOMI DATA\n",
    "base_path = '/home/jovyan/efs/eis-fire-tropomi'\n",
    "tropomi_filepath = {\n",
    "    'TROPOMI AEROSOL-INDEX': \"tropomi_aer_ai.zarr\",\n",
    "    'TROPOMI CO': 'tropomi_co.zarr',\n",
    "}\n",
    "for k, v in tropomi_filepath.items():\n",
    "    tropomi_filepath[k] = os.path.join(base_path, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined variables\n",
    "\n",
    "Add, modify, or remove variables below. Please follow conventions outlined in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Variables from raster datasets to use.\n",
    "# Follow {'DATASET': ['var1', 'var2'],\n",
    "#         'DATASET': ['var1', 'var2']\n",
    "# ---\n",
    "raster_variables = {\n",
    "    'IMERG FWI': ['IMERG.FINAL.v6_FWI'],\n",
    "    'GEOS FP CONUS': ['T2M'],\n",
    "    'QFED': ['co.biomass'],\n",
    "    'GEOS FWI': ['GEOS-5_FWI'],\n",
    "}\n",
    "\n",
    "\n",
    "raster_variable_to_show = 'IMERG.FINAL.v6_FWI' #'T2M'\n",
    "\n",
    "# If desired, slice time to date_start and date_end.\n",
    "date_start = '2020-06-01'\n",
    "date_end = '2020-11-30'\n",
    "d_s = datetime(2020, 6, 1)\n",
    "d_e = datetime(2020, 11, 30)\n",
    "\n",
    "\n",
    "# See ____ for more projection options.\n",
    "projection = ccrs.PlateCarree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common iterables\n",
    "tbounds = slice(date_start, date_end)\n",
    "line_color_iter = ['red', 'blue', 'green', 'black', 'indianred', 'grey', 'maroon', 'orange', 'gold', 'darkgreen', 'darkslategrey', \n",
    "                  'steelblue', 'purple', 'crimson']\n",
    "cmaps  = {n: cc.palette[n] for n in ['kbc', 'fire', 'bgy', 'bgyw', 'bmy', 'gray', 'kbc']}\n",
    "cmaps_str = ['kbc', 'fire', 'bgy', 'bgyw', 'bmy', 'gray']\n",
    "\n",
    "# Valid data are masks 10-15, 20-25, 30-35\n",
    "goesmasks = tuple(list(range(10, 16)) + list(range(20, 26)) + list(range(30, 36)))\n",
    "\n",
    "# Read in shape files\n",
    "psa_shp = gpd.read_file('./shape_files/NIFC_PSA/National_Predictive_Service_Areas_(PSA)_Boundaries.shp')\n",
    "fires = gpd.read_file('./shape_files/YANG_CA_FIRES/yang-ca-fires.json')\n",
    "\n",
    "print('Loading in and merging all datasets')\n",
    "# Map to S3 location\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "dataset_path = {}\n",
    "for dataset, path in raster_filepath.items():\n",
    "    print(dataset, path)\n",
    "    dataset_path[dataset] = s3.get_mapper(path)\n",
    "\n",
    "zarrDict = {}\n",
    "for mission, s3_file in dataset_path.items():\n",
    "    print(mission, s3_file)\n",
    "    try:\n",
    "        zarrDict[mission] = xr.open_zarr(s3_file, consolidated=True)\n",
    "    except:\n",
    "        zarrDict[mission] = xr.open_zarr(s3_file, consolidated=False)\n",
    "datasetSubs = {}\n",
    "rasterAttrs = {}\n",
    "for mission, dataset in raster_variables.items():\n",
    "    for subdataset in dataset:\n",
    "        if mission not in datasetSubs.keys():\n",
    "            datasetSubs[mission] = {}\n",
    "        rasterAttrs[subdataset] = zarrDict[mission][subdataset].attrs\n",
    "        rasterAttrs[subdataset]['mission'] = mission\n",
    "        datasetSubs[mission].update({ subdataset : zarrDict[mission][subdataset]})\n",
    "        \n",
    "#---\n",
    "# Dataset:subdataset resampling to daily temporal resolution\n",
    "#---\n",
    "datasetSubs['GEOS FP CONUS']['T2M'] = datasetSubs['GEOS FP CONUS']['T2M'].resample(time='1D').mean(\"time\").sel(time=tbounds)\n",
    "datasetSubs['GEOS FWI']['GEOS-5_FWI'] = datasetSubs['GEOS FWI']['GEOS-5_FWI'].resample(time='1D').mean('time').sel(time=tbounds).interp_like(datasetSubs['GEOS FP CONUS']['T2M']).drop('forecast_lag')\n",
    "datasetSubs['IMERG FWI']['IMERG.FINAL.v6_FWI'] = datasetSubs['IMERG FWI']['IMERG.FINAL.v6_FWI'].sel(time=tbounds).interp_like(datasetSubs['GEOS FP CONUS']['T2M'])\n",
    "datasetSubs['QFED']['co.biomass'] = datasetSubs['QFED']['co.biomass'].sel(time=tbounds).interp_like(datasetSubs['GEOS FP CONUS']['T2M'])\n",
    "\n",
    "mergedDataset = None\n",
    "for mission, dataDict in datasetSubs.items():\n",
    "    for subd, array in dataDict.items():\n",
    "        if mergedDataset is None:\n",
    "            mergedDataset = array\n",
    "        print(subd)\n",
    "        mergedDataset = xr.merge([mergedDataset, array])\n",
    "datasetSubs = None\n",
    "\n",
    "# ---\n",
    "# Handling TROPOMI data\n",
    "# ---\n",
    "print('Opening and merging all TROPOMI data.')\n",
    "tropomiDict = {}\n",
    "for mission, path in tropomi_filepath.items():\n",
    "    print(mission, path)\n",
    "    tropomiDict[mission] = xr.open_zarr(path)\n",
    "\n",
    "tropomiVars = {}\n",
    "for ds in tropomiDict.keys():\n",
    "    l = list(tropomiDict[ds].variables)\n",
    "    l.remove('lat')\n",
    "    l.remove('lon')\n",
    "    l.remove('time')\n",
    "    tropomiVars[ds] = l\n",
    "\n",
    "tropomiSubs = {}\n",
    "for mission, dataset in tropomiVars.items():\n",
    "    for subdataset in dataset:\n",
    "        if mission not in tropomiSubs.keys():\n",
    "            tropomiSubs[mission] = {}\n",
    "        rasterAttrs[subdataset] = tropomiDict[mission][subdataset].attrs\n",
    "        rasterAttrs[subdataset]['mission'] = mission\n",
    "        tropomiSubs[mission].update({ subdataset : tropomiDict[mission][subdataset]})\n",
    "\n",
    "for k in tropomiSubs.keys():\n",
    "    for v in tropomiSubs[k]:\n",
    "        tropomiSubs[k][v] = tropomiSubs[k][v].resample(time='1D').mean('time').sel(time=tbounds)\n",
    "\n",
    "tropoDataset = None\n",
    "for mission, dataDict in tropomiSubs.items():\n",
    "    for subd, array in dataDict.items():\n",
    "        if tropoDataset is None:\n",
    "            tropoDataset = array\n",
    "        tropoDataset = xr.merge([tropoDataset, array])\n",
    "tropomiSubs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to query and read shape layers and data\n",
    "General helper functions included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructDataDict(mergedDataset=None, attrsDict=None):\n",
    "    \"\"\"Construct a meta-data dictionary to accompany all dataset:variable combinations\"\"\"\n",
    "    dataDict = dict()\n",
    "    variableList = list(mergedDataset.variables)[3:] if mergedDataset else attrsDict.keys()\n",
    "    varListForSum = ['co', 'co.biomass']\n",
    "    for var in variableList:\n",
    "        if mergedDataset:\n",
    "            metadata = mergedDataset[var].attrs\n",
    "        else:\n",
    "            metadata = attrsDict[var]\n",
    "        try:\n",
    "            longName = '{}:{}'.format(metadata['long_name'], metadata['mission'])\n",
    "        except KeyError:\n",
    "            longName = '{}:{}'.format(var, metadata['mission'])\n",
    "        try:\n",
    "            units = metadata['units']\n",
    "        except KeyError:\n",
    "            units = ''\n",
    "        needsCollapse = True if var in varListForSum else False\n",
    "        shortName = var\n",
    "        tropomiSet = True if var in list(tropoDataset.variables)[3:] else False\n",
    "        dataDict[longName] = {\n",
    "            'metadata': metadata,\n",
    "            'units': units,\n",
    "            'collapse': needsCollapse,\n",
    "            'shortName': shortName,\n",
    "            'tropomiSet': tropomiSet,\n",
    "            'derived': False\n",
    "        }\n",
    "    return dataDict\n",
    "\n",
    "def clip_to_shape(raster, shape_file):\n",
    "    \"\"\"Given a raster and geometry, clip raster to the given geometry.\"\"\"\n",
    "    raster.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    raster.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "    raster_clipped = raster.rio.clip(shape_file.geometry.apply(mapping), shape_file.crs, drop=False)\n",
    "    return raster_clipped\n",
    "\n",
    "def collapseTo1D(datarray):\n",
    "    \"\"\"Collapse lat/lon via sum given an xarray.\"\"\"\n",
    "    dataCollapsed = datarray.sum(axis=1)\n",
    "    dataCollapsed = dataCollapsed.sum(axis=1)\n",
    "    return dataCollapsed\n",
    "\n",
    "def collapseMean(datarray):\n",
    "    \"\"\"Load dat and collapse to averaged lat/lon\"\"\"\n",
    "    datarray = datarray.load()\n",
    "    dataCollapsed = datarray.mean(['lat'])\n",
    "    dataCollapsed = dataCollapsed.mean(['lon'])\n",
    "    return dataCollapsed\n",
    "\n",
    "def plotTS(datarray, lat, lon):\n",
    "    \"\"\"Plot time-series given a xarray dataarray and lat/lon\"\"\"\n",
    "    datarray = datarray.load()\n",
    "    dataSelected = datarray.interactive.sel(lon=lon, \n",
    "                                            lat=lat, \n",
    "                                            method='nearest')\n",
    "    return dataSelected\n",
    "\n",
    "def updateTitle(lat, lon):\n",
    "    latStr = str(round(lat, 4))\n",
    "    lonStr = str(round(lon, 4))\n",
    "    mkdStr = '## Point Selected: ({}, {})'.format(lonStr, latStr)\n",
    "    return pn.pane.Markdown(mkdStr, width=800)\n",
    "\n",
    "def timeAveragedCallBack(target, event):\n",
    "    \"\"\"Callback to enable time-averaged related widgets\"\"\"\n",
    "    target.disabled = False if event.new == 'Time Averaged' else True\n",
    "\n",
    "def sequentialDisable(target, event):\n",
    "    \"\"\"Callback to disable time-averaged related widgets\"\"\"\n",
    "    target.disabled = True if event.new == 'Time Averaged' else False\n",
    "\n",
    "def aeSitesCallback(target, event):\n",
    "    \"\"\"Callback to enable Aeronet-related widgets\"\"\"\n",
    "    target.disabled = False if event.new == True else True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contruct metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataDict = constructDataDict(mergedDataset=None, attrsDict=rasterAttrs)\n",
    "totalVars = list(dataDict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechunk and persist data to memory\n",
    "\n",
    "With added derived datasets will take 10 minutes to persist data to memory. 2 minutes without derived datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mergedDataset = mergedDataset.chunk({'time':-1})\n",
    "tropoDataset = tropoDataset.chunk({'time':-1})\n",
    "print('Peristing merged data')\n",
    "mergedDataset = mergedDataset.persist()\n",
    "print('Persisting derived data')\n",
    "# Comment below line out to not persist derived vars\n",
    "latLonGrid = tropoDataset['aerosol_index_354_388'].isel(time=0)\n",
    "latLonGrid = latLonGrid.rename('pl')\n",
    "latLonGrid = latLonGrid * 0\n",
    "latLonGrid = latLonGrid.persist()\n",
    "print('Persisting TROPOMI data')\n",
    "tropoDataset = tropoDataset.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization 1 - Interactive time series\n",
    "\n",
    "This is a basic interactive plot. The control panel on the left give the users many options, including choosing colormaps, which state to get data from, and more.\n",
    "\n",
    "The controls on the right allow you to zoom, move around, and save the visual. Hover over the visual to see variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeAveragedSequentialRadioGroup = pn.widgets.RadioButtonGroup(\n",
    " name='Visualization Type', options=['Daily', 'Time Averaged'], value='Daily', button_type='success')\n",
    "\n",
    "mapLayers= pn.widgets.RadioButtonGroup(\n",
    "    name='Base map layer',\n",
    "    options=['Open Street Map', 'Satellite Imagery'],\n",
    "    value='Satellite Imagery',\n",
    "    button_type='success')\n",
    "\n",
    "bufferInput = pn.widgets.FloatInput(name='Buffer (degrees)', value=0.1)\n",
    "timeStepInput = pn.widgets.TextInput(name='Time Step', value='7D', disabled=True)\n",
    "timeAveragedSequentialRadioGroup.link(timeStepInput, callbacks={'value': timeAveragedCallBack})\n",
    "toggleCSVExport = pn.widgets.Button(name='Export to time series to CSV', button_type='success')\n",
    "\n",
    "# List desired defualt variables here\n",
    "defaultVars = ['IMERG.FINAL.v6 Fire Weather Index:IMERG FWI',\n",
    "               'Vertically integrated CO column:TROPOMI CO', \n",
    "               'CO2 Biomass Emissions:QFED']\n",
    "addTimeSeries = pn.widgets.MultiChoice(name='Time Series Variables',  value=defaultVars, options=totalVars)\n",
    "\n",
    "# Edit title and subtitles here\n",
    "titleText = '# EIS FIRE - Mult-variable time series visualization'\n",
    "subtitleText = 'This interactive dashboard displays raster data in an interactive format.' + \\\n",
    "' This raster data may be clipped to individual states via user-controls. '+ \\\n",
    "'In addition this dashboard displays various time-series of user-given data products. '+ \\\n",
    "'These time-series are averaged over individual state polygons. All TROPOMI data is unfiltered L2 gridded data' + \\\n",
    "'\\n <b>Make sure dashboard is done loading before making changed to left-side control bar</b>'\n",
    "title = pn.pane.Markdown(titleText, width=600)\n",
    "subtitle = pn.pane.Markdown(subtitleText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(addTS=addTimeSeries.param.value, exportToCSV=toggleCSVExport.param.value, rasterType=timeAveragedSequentialRadioGroup.param.value,\n",
    "            step=timeStepInput, mapLayer=mapLayers.param.value, buffer=bufferInput.param.value)\n",
    "def visuaPanel(addTS, exportToCSV, rasterType, step, buffer, mapLayer):\n",
    "    timeAve = True if rasterType=='Time Averaged' else False\n",
    "    tile = 'OSM' if mapLayer=='Open Street Map' else 'ESRI' # Choose basemap style. \n",
    "    data = ([40], [-110]) # Starting point selection.\n",
    "    points = hv.Points(data).redim.range(x=(-130, -70), y=(0, 60)) # Limit to U.S.\n",
    "    \n",
    "    # Captures user-input and plots onto the map. No functionality attached. \n",
    "    points = points.opts(active_tools=['point_draw'], \n",
    "                         color='red',\n",
    "                         size=10, \n",
    "                         marker='^')\n",
    "    point_stream = streams.PointDraw(data=points.columns(), num_objects=1, source=points) # Captures user input for marker.\n",
    "    \n",
    "    # Plotting basemap based off of empty xarray to give us a nice lat/lon grid with ESRI tiles.\n",
    "    baseMap = latLonGrid.hvplot(alpha=0, \n",
    "                                xlim=(-130, -70), \n",
    "                                ylim=(0, 60), \n",
    "                                frame_width=800, \n",
    "                                tiles=tile, \n",
    "                                frame_height=600, \n",
    "                                coastline=True, \n",
    "                                colorbar=False).opts(opts.Image(xlabel='Longitude',\n",
    "                                                                ylabel='Latitude'))\n",
    "    overlay = (baseMap * points)\n",
    "    \n",
    "    #Captures user input -120.8762, 37.6069\n",
    "    stream = hv.streams.Tap(source=baseMap.Image.I, x=-120.8762, y=37.6069) \n",
    "    \n",
    "    # Plotting time series\n",
    "    col = pn.Column(pn.Card(overlay))\n",
    "    \n",
    "    # Make a nice grid to populate with time series.\n",
    "    rowDict= {}\n",
    "    tsCard = pn.Card()\n",
    "    tsCard.append(pn.bind(updateTitle, lat=stream.param.y, lon=stream.param.x))\n",
    "    \n",
    "    for i in range(20):\n",
    "        row = pn.Row()\n",
    "        rowDict[i+1] = row\n",
    "        tsCard.append(row)\n",
    "    \n",
    "    for i, ts in enumerate(addTS):\n",
    "        rowIdx = ((i+1) + 2 // 2) // 2 # Find which row to put the TS in. \n",
    "        color=line_color_iter[random.randint(0, len(line_color_iter)-1)]\n",
    "        if (not 'aod' in ts): # Make sure we aren't plotting any Aeronet data yet. \n",
    "            varShortName = dataDict[ts]['shortName'] # Variable short name from which to index with.\n",
    "            tropo = dataDict[ts]['tropomiSet'] # A bool to let us know if it's tropomi data.\n",
    "            derived = dataDict[ts]['derived'] # If it's a derived dataset. \n",
    "            \n",
    "            if tropo:\n",
    "                rasterTS = tropoDataset[varShortName]\n",
    "            elif(derived):\n",
    "                rasterTS = derivedDataset[varShortName]\n",
    "            else:\n",
    "                rasterTS = mergedDataset[varShortName]\n",
    "            \n",
    "            rasterTS = rasterTS.resample(time=step).mean(dim='time') if timeAve else rasterTS # Resample to user-given time-step.\n",
    "            ds = plotTS(rasterTS, lat=stream.param.y, lon=stream.param.x) # Dataset given a raster and lat, lon. \n",
    "            ylabel = '{} ({})'.format(varShortName, dataDict[ts]['units'])\n",
    "            addTimeSeriesPlot = ds.hvplot('time', \n",
    "                                          color=color, \n",
    "                                          title='{} {}'.format(ts, dataDict[ts]['units']),\n",
    "                                         ylabel=ylabel)\n",
    "            \n",
    "            if exportToCSV:\n",
    "                ds.to_pandas().to_csv('{}.csv'.format(varShortName))\n",
    "            \n",
    "            rowDict[rowIdx].append(addTimeSeriesPlot.dmap())\n",
    "    \n",
    "    col.append(tsCard)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleRow = pn.Row(pn.Column(title, subtitle))\n",
    "accordion = pn.Accordion(\n",
    "                    ('Layers', pn.Column(mapLayers)),\n",
    "    header_background='#0059b3', header_color='white', active_header_background='#339966')\n",
    "widgetBox = pn.WidgetBox(pn.Column(timeAveragedSequentialRadioGroup, \n",
    "                                accordion,\n",
    "                                bufferInput,\n",
    "                                timeStepInput, \n",
    "                                addTimeSeries, toggleCSVExport), height=800)\n",
    "dashboard = pn.Column(titleRow, pn.Row(widgetBox, visuaPanel), background='WhiteSmoke')\n",
    "print('Rendering')\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
